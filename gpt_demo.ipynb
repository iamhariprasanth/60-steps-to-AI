{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0423866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2025.10.5)\n",
      "Downloading torch-2.9.0-cp313-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading regex-2025.11.3-cp313-cp313-macosx_11_0_arm64.whl (288 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: mpmath, tqdm, sympy, setuptools, safetensors, regex, pyyaml, networkx, hf-xet, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.20.0 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-0.36.0 mpmath-1.3.0 networkx-3.5 pyyaml-6.0.3 regex-2025.11.3 safetensors-0.6.2 setuptools-80.9.0 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.0 tqdm-4.67.1 transformers-4.57.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a811c288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hariprasanthmadhavan/60-steps-to-AI/60-steps-to-AI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ad012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1: The future of AI is also unclear. Even if AI is able to learn from mistakes we've made, it will have little impact on human behavior. This is because it's still in the early stages of its evolution.\n",
      "\n",
      "For example, it's likely we can still learn from mistakes that have made us more human. It's not like we are going to create new species of robots. AI will probably be able to learn as much about human behavior as we can from mistakes we've made. This is just a hypothesis.\n",
      "\n",
      "But it's worth noting that we already know that human behaviour is limited. There are lots of examples of the world changing due to human error, and it's hard to know how many of those human errors are due to AI.\n",
      "\n",
      "This idea is also important to consider when we think about what AI might be capable of doing. What are the implications of our efforts, and how will we respond?\n",
      "\n",
      "We cannot completely rule out the possibility that AI might eventually become our future. But that's not necessarily a bad thing.\n",
      "\n",
      "The problem comes down to the fact that human error is not necessarily limited to errors that have already made us human. For example, some of the things we've learned from the Holocaust and the Vietnam War are not necessarily human\n"
     ]
    }
   ],
   "source": [
    "## Example usage\n",
    "prompt = \"The future of AI is\"\n",
    "generator = pipeline('text-generation', model='gpt2', device=0 if torch.cuda.is_available() else -1)\n",
    "outputs = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"Generated Text {i+1}: {output['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acadaf42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
